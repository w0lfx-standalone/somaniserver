# v1.2 â€“ TrueNAS Setup and Pool Management  

## Objective  
With Proxmox and the core VMs in place, the next step was to configure **TrueNAS SCALE** as the storage backbone of the system.  
The goals of this stage were:  
- Create a redundant ZFS pool for data reliability  
- Organize storage into service-specific datasets  
- Configure NFS shares to integrate with the Ubuntu VM  
- Standardize permissions via UID/GID mapping  

## TrueNAS Installation  
- Installed TrueNAS SCALE into its dedicated VM (80GB boot disk, 1 vCPU, 5.5GB RAM)  
- Verified LAN connectivity through bridged networking  
- Applied latest system updates  

![TrueNAS Dashboard](https://github.com/w0lfx-standalone/somaniserver/blob/main/assets/truenas_dashboard.png)


## Pool Creation  
- Used the 2Ã—2TB HDDs (passed through from Proxmox) to create a **ZFS mirror pool** named `swimming_pool`  
- Verified resiliency: single disk failure still keeps data safe  
- Enabled basic pool health monitoring in TrueNAS  

![TrueNAS Pool Dashboard](https://github.com/w0lfx-standalone/somaniserver/blob/main/assets/truenas_pool.png)

## Dataset Structure  
### Created dedicated datasets for modular service isolation:  
- `immich/` â€“ Photo & video backups  
- `nextcloud/` â€“ File syncing and collaboration  
- `seafile/` â€“ Lightweight versioning + sync   
- `media/` â€“ Shared general storage  

![TrueNAS Dataset](https://github.com/w0lfx-standalone/somaniserver/blob/main/assets/truenas_datasets.png)

## Sharing & Permissions  
- Configured **NFS shares** for each dataset  
- Created `somaniserver` (UID 3000) and `somanigroup` (GID 3001) on both TrueNAS and Ubuntu for permission consistency  
- Applied POSIX ACLs so all datasets are owned by `3000:3001`  
- Mounted each dataset on Ubuntu and tested read/write  

![TrueNAS NFS](https://github.com/w0lfx-standalone/somaniserver/blob/main/assets/truenas_nfsconfig.png)

![Ubuntu Mount](https://github.com/w0lfx-standalone/somaniserver/blob/main/assets/ubuntu_ownership_and_mounts.png)
## Verification & Testing  
- Performed file creation and deletion tests from Ubuntu â†’ TrueNAS datasets  
- Confirmed UID/GID mapping worked correctly (`ls -l` showed `somaniserver:somanigroup`)  
- Tested NFS performance with sample transfers  

![Ubuntu Mount test](https://github.com/w0lfx-standalone/somaniserver/blob/main/assets/ubuntu_mounttest.png)

---

## Reliability & Recovery Confidence  
A big reason for choosing **TrueNAS SCALE with ZFS** is the way it handles failures gracefully and makes recovery straightforward.  

- **Disk failure isnâ€™t catastrophic**  
  In a ZFS mirror, data is written to both drives. If one disk fails, the other still contains a complete copy. The system keeps running normally, and replacing the failed drive triggers a *resilver* process â€” ZFS rebuilds the new disk by copying data from the healthy one, restoring redundancy with no downtime.  

- **Resilvering is efficient**  
  Unlike traditional RAID, ZFS only rebuilds the blocks that are actually in use, not the entire raw disk. This means resilvering is faster and puts less stress on the surviving drive, reducing the chance of a second failure during recovery.  

- **Recovery doesnâ€™t depend on the OS**  
  All critical metadata â€” datasets, snapshots, and numeric UID/GID permissions â€” are stored *on the pool itself*, not just in TrueNAS. This means even if the TrueNAS VM or configuration file (`.db`) is lost, the pool can still be imported on a fresh installation.  
  - If the config file is available â†’ recovery is seamless, with users, groups, and shares restored.  
  - If not â†’ I can manually recreate users/groups (UID 3000 / GID 3001) and NFS shares, and the datasets remain intact.  

- **Future-proofing with snapshots**  
  ZFS supports lightweight snapshots, meaning I can later roll back datasets to earlier states. Even if something goes wrong with a service, the underlying data remains safe.  

- **Why this matters for my setup**  
  By building the `swimming_pool` mirror and standardizing UID/GID mapping, I know that:  
  - A single drive failure wonâ€™t stop the server.  
  - Replacing a drive is routine, not a crisis.  
  - Even in a full system reinstall, my pool can be imported and my data remains usable.  

This turns TrueNAS from just a storage manager into a **resilient backbone** for the whole project, where data integrity and recovery are guaranteed by design.  

---


## ðŸ”„ Version Note  
This milestone establishes the **storage foundation** for somaniserver.  
- Redundancy is ensured through a ZFS mirror.  
- Datasets are modular and isolated per service.  
- NFS integration and UID/GID mapping allow seamless access from the Ubuntu VM.  

With storage now reliable and centralized, the project is ready to move forward to **v1.3: Setting up Immich**.  
